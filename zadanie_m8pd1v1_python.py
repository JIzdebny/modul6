# -*- coding: utf-8 -*-
"""zadanie_M8PD1V1-Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SvUR05sxTD_BMp1nzUDmZoOsw0e0jwJd

# Modul 8 - Praca domowa - Python

Zwykle istnieje wiele sposobów, żeby rozwiązać zadanie w Python. Także czasami warto spróbować opracować kod na 1-2 sposoby :)

## Zadanie - 1

<img src="https://media.giphy.com/media/AgHBbekqDik0g/giphy.gif" width="800">

> Wykorzystaj platwormę Bankier.pl do pobrania tabeli z notowaniami [[Źródło Danych](https://bit.ly/3boWGJ7)]. Dane dotyczą notowań cen akcji na GPW. Zescrapuj całą tabelę z notowaniami wykorzystując:

>**1. Podejście - biblioteki: `requests` oraz `BeautifulSoup`**
"""

# import bibliotek
import requests
from bs4 import BeautifulSoup
import pandas as pd

# tworzenie żądania typu GET
URL = "https://www.bankier.pl/gielda/notowania/akcje"
zwrotna_odpowiedz = requests.get(URL)
print(zwrotna_odpowiedz)

# pobranie zawartości HTML
html = zwrotna_odpowiedz.content
print(html)

# "Gotowanie" - tworzenie obiektu SOUP
zupa = BeautifulSoup(html, "html5lib")
print(zupa)

# zapisywanie HTML
with open("bankier.html", "wb") as plik:
  plik.write(zupa.prettify(encoding="utf-8"))

len(zupa.find_all("table"))

# pierwsza tabela
tabela = zupa.find("table")
tabela

# wyświetlenie elementów podrzędnych tabeli - dzieci
tabela.contents

# wyświetlenie rodzica tabeli
tabela.parent

# długość listy elementów tabeli
len(tabela.contents)

# piewszy wiersz tabeli
tabela.find_all("tr")[0].contents

zupa.find("thead")

tabela_nazwa = []

tabela_kolumna = zupa.find("thead")
for nazwa_kolumny in tabela_kolumna.find_all("th"):
  tabela_nazwa.append(nazwa_kolumny.text.strip())

print(tabela_nazwa)

tabela_dane = []
linie_tab = []

for linia in tabela.find_all("tr")[1:]:
  komorka = linia.find_all("td")
  linie_tab = []
  for temp in komorka:
    linie_tab.append(temp.text.strip().replace(' ','').replace(',', '.'))
  tabela_dane.append(linie_tab)
print(tabela_dane[0])

df = pd.DataFrame(tabela_dane)
df.columns = tabela_nazwa

df.count

df.head()

""">**2. Podejście - biblioteka: `pandas`**"""

# użycie Pandas do pobrania tabeli
import pandas as pd

tabela1_pd = pd.read_html(URL)
tabela1_pd

""">**Zdefiniuj odpowiedzi na pytania:**
* Która z akcji obecnie jest najdroższa na GPW?
* Top 5 akcji, które odnotowały największy wzrost wartości akcji w ostatnim okresie?
* Top 5 akcji, które odnotowały największy spadek wartości akcji w ostatnim okresie?
* Inne przydatne informacje (*opcjonalnie*)
"""

df['Kurs AD'] = pd.to_numeric(df['Kurs AD'], errors ='coerce')
akcja_najdrozsza = df[df['Kurs AD'] == df['Kurs AD'].max()]
akcja_najdrozsza

df['Max AD'] = pd.to_numeric(df['Max AD'], errors='coerce')
df.sort_values(by ='Max AD', ascending=False, inplace=True)
df.head(5)

df['Min AD'] = pd.to_numeric(df['Min AD'], errors ='coerce')
df.sort_values(by ='Min AD', ascending=False, inplace=True)
df.head(5)

"""## Zadanie - 2

<img src="https://media.giphy.com/media/AbDCwAI2xTwTm/giphy.gif" width="500">

> Wykorzystaj platwormę Filmweb.pl do pobrania informacji o najlepszych 25 filmach z rankingu 500 filmów z całego świata [[Źródło Danych](https://bit.ly/3bnWsSF)]. Strona zawiera informacje: pozycja w rankinhu, nazwa filmu po polsku, nazwa filmu po angielsku, rok premiery, średnia ocena, liczba ocen, itd. Zrealizuj kolejne zadania:

>1. Zescrapuj 25 pierwszych filmów z rankingu, pobierając informacje o: pozycja w rankinhu, nazwa filmu po polsku, nazwa filmu po angielsku, rok premiery, średnia ocena, liczba ocen, link do miniaturki, link do podstrony.
"""

# import bibliotek
import requests
from bs4 import BeautifulSoup
import pandas as pd
# tworzenie żądania typu GET
URL = "https://www.filmweb.pl/ranking/film"
zwrotna_odpowiedz = requests.get(URL)
print(zwrotna_odpowiedz)

# pobranie zawartości HTML
html = zwrotna_odpowiedz.content
# "Gotowanie" - tworzenie obiektu SOUP
zupa = BeautifulSoup(html, "html.parser")
print(zupa)

# zapisywanie HTML
with open("filmy.html", "wb") as plik:
  plik.write(zupa.prettify(encoding="utf-8"))

# tworzę listę filmów
filmy = zupa.find_all("div", {"class": "item place "})
filmy

# tworzenie całego linku
from urllib.parse import urljoin
print(URL)

#zupa.find_all("div", {"id":"siteSub", "class":"noprint"})
temp = filmy[3].find("div", {"itemprop": "director"}).text
print(temp)

film_ID = filmy[0].find("div",{"class": "ranking__position"}).text
film_nazwa = filmy[0].find("div", {"class": "film__original"}).text
film_nazwa_pol = filmy[0].find("span",{"itemprop": "name"}).text
film_ocena = filmy[0].find("span", {"class": "rate__value"}).text
film_liczba_oceniajacych = filmy[0].find("span", {"class": "rate__count"}).text.strip(" ocen").strip(" oceny")
film_rok = filmy[0].find("span", {"class": "film__production-year"}).text


# tworzenie linków do podstron
druga_czesc_linku = filmy[0].find("a").get("href")
caly_link = urljoin(URL, druga_czesc_linku)

druga_czesc_linku_2 = filmy[0].find("a", {"class": "film__link"}).get("href")
caly_link_2 = urljoin(URL, druga_czesc_linku_2)

print(film_ID)
print(film_nazwa)
print(film_nazwa_pol)
print(film_ocena)
print(film_liczba_oceniajacych)
print(film_rok)
print(caly_link)
print(caly_link_2)

# tworzenie tablic do zapisu danych
link_1 = []
link_2 = []
film_ID = []
film_nazwa = []
film_nazwa_pol = []
film_ocena = []
film_liczba_oceniajacych = []
film_rok = []


for film in filmy:
  get_film = film.find("div", {"class": "ranking__position"}).text

  get_film_nazwa = film.find("div", {"class": "film__original"})
  get_film_nazwa_pol = film.find("span",{"itemprop": "name"}).text
  get_film_ocena = film.find("span", {"class": "rate__value"}).text
  get_film_liczba_oceniajacych = film.find("span", {"class": "rate__count"}).text
  get_film_rok = film.find("span", {"class": "film__production-year"}).text


  get_czesc_linku = film.find("a").get("href")
  get_link = urljoin(URL, get_czesc_linku)

  _czesc_linku_2 = film.find("a", {"class": "film__link"}).get("href")
  get_link2 = urljoin(URL, _czesc_linku_2)

  film_ID.append(get_film)
  film_nazwa.append(get_film_nazwa)
  film_nazwa_pol.append(get_film_nazwa_pol)
  film_ocena.append(get_film_ocena)
  film_liczba_oceniajacych.append(get_film_liczba_oceniajacych)
  film_rok.append(get_film_rok)
  link_1.append(get_link)
  link_2.append(get_link2)

""">2. Stwórz DF oraz uzupełnij pobrane linki, jeżeli nie są pobrane w całości."""

# Szerokość kolumny pandas
pd.set_option('display.max_colwidth', 550)

# Tworzenie zestawu danych do DF:
lista_kolumn = [film_ID, film_nazwa_pol, film_nazwa, film_rok, film_ocena, film_liczba_oceniajacych, link_1, link_2]
nazwa_kolumn = ['Nr. w ranking', 'Nazwa filmu', 'Orginalna nazwa filmu', 'Data premiery', 'OCENA', 'Liczba ocen', 'LINK', 'LINK 2']

# Tworzenie DF
DF = pd.DataFrame({"Nr. w rankingu" : film_ID,
                          "Nazwa filmu"  : film_nazwa_pol,
                          "Orginalna nazwa filmu" : film_nazwa,
                          "Data premiery" : film_rok,
                          "OCENA" : film_ocena,
                          "Liczba ocen" : film_liczba_oceniajacych,
                          "LINK" : link_1,
                          "LINK 2" : link_2})



DF.head()

""">3. Wykorzystując linki do podstron z listy pobranych filmów, uzupełnij stworzony DF o informacje: opis filmu, informacje o nagrodach, czas trwania filmu, reżyser, scenariusz, gatunek, produkcja."""

link_2

opis = []
nagrody = []
czas_min = []
rezyser = []
scenariusz = []
gatunek = []
produkcja = []



for strona in link_2:

  strona_odpowiedz = requests.get(strona)
  strona_html = strona_odpowiedz.content
  strona_zupa_obiekt = BeautifulSoup(strona_html, "lxml")
  #strona_zupa_obiekt = strona_zupa.find_all("div", attrs={"class":"filmPosterSection__container page__text"})


  strona_opis = strona_zupa_obiekt.find("div", {"class": "filmPosterSection__plot"})
  
  strona_nagrody = strona_zupa_obiekt.find("div", {"class": "filmInfo__awards"}).text.strip(" ")
  strona_czas_min = strona_zupa_obiekt.find("span", {"itemprop":"timeRequired"}).get("data-duration")
  strona_rezyser = strona_zupa_obiekt.find("div", {"data-type": "directing-info"}).text.strip(" ")
  strona_scenariusz = strona_zupa_obiekt.find("div", {"data-type": "screenwriting-info"}).text.strip("/ więcej...")
  strona_gatunek = strona_zupa_obiekt.find("div", {"itemprop": "genre"}).text.strip(" ")
  strona_produkcja = strona_zupa_obiekt.find(text="produkcja").next_element.find("a").text

  opis.append(strona_opis)
  nagrody.append(strona_nagrody)
  czas_min.append(strona_czas_min)
  rezyser.append(strona_rezyser)
  scenariusz.append(strona_scenariusz)
  gatunek.append(strona_gatunek)
  produkcja.append(strona_produkcja)

print(rezyser)

DF_2 = pd.DataFrame({"Nr. w rankingu" : film_ID,
                          "Nazwa filmu"  : film_nazwa_pol,
                          "Orginalna nazwa filmu" : film_nazwa,
                          "Data premiery" : film_rok,
                          "OCENA" : film_ocena,
                          "Liczba ocen" : film_liczba_oceniajacych,
                          "LINK" : link,
                          "LINK 2" : link_2,
                          "OPIS" : opis,
                          "Nagrody" : nagrody,
                          "Czas min" : czas_min,
                          "Rezyser" : rezyser,
                          "Scenariusz" : scenariusz,
                          "Gatunek" : gatunek,
                          "Produkcja" : produkcja})

DF_2.head()

""">4. Przekształć czas trwania filmu w minuty."""

DF_2["Czas min"] = pd.to_numeric(DF_2["Czas min"])
DF_2.dtypes

DF_2["Czas min"]

""">5. Jaka średnia długość filmów w TOP25?"""

print("Średnia długość filmów w minutach wynosi:", DF_2["Czas min"].mean())

""">6. Jakiej produkcji są najczęściej filmy w TOP25?"""

DF_2["Produkcja"].value_counts()

""">7. Filmy jakiej produkcji dostawały Oscar'a? Ile razy wg. TOP25?"""

DF_2[DF_2["Nagrody"].str.contains("Oscar")]

""">8. Inne ciekawe analizy na podstawie stworzonego zestawu danych (*opcjonalnie*)."""

opcjonalnie = DF_2.sort_values(by=["Czas min"], ascending=False)
opcjonalnie.loc[:, ['Nazwa filmu' ,'Czas min']]

""">9. Podziel się wynikiem w grupie na [FB](https://bit.ly/2OSyHaG) podając hashtag `#modul8`."""